{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e59732",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Load Learning Curve Data\n",
    "data = pd.read_csv('data/learning_curves.csv')\n",
    "degrees = data['degree'].values\n",
    "train_err = data['train_error'].values\n",
    "test_err = data['test_error'].values\n",
    "\n",
    "# 2. Visualizing the Bias-Variance Tradeoff\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(degrees, train_err, 'o-', label='Training Error (Bias Indicator)', color='blue')\n",
    "plt.plot(degrees, test_err, 's-', label='Test Error (Variance Indicator)', color='red')\n",
    "\n",
    "# Highlighting the \"Sweet Spot\"\n",
    "optimal_idx = np.argmin(test_err)\n",
    "plt.axvline(x=degrees[optimal_idx], linestyle='--', color='green', label='Optimal Complexity')\n",
    "\n",
    "plt.xlabel('Model Complexity (Polynomial Degree)')\n",
    "plt.ylabel('Error Rate')\n",
    "plt.title('Autonomous Systems: Bias vs. Variance Analysis')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# 3. Sample Complexity Calculation\n",
    "def calculate_sample_complexity(k, gamma, delta):\n",
    "    \"\"\"\n",
    "    Computes required m for uniform convergence.\n",
    "    k: size of hypothesis class\n",
    "    gamma: error tolerance\n",
    "    delta: failure probability\n",
    "    \"\"\"\n",
    "    return (1 / (2 * gamma**2)) * np.log(2 * k / delta)\n",
    "\n",
    "m_required = calculate_sample_complexity(k=1000, gamma=0.05, delta=0.01)\n",
    "print(f\"Required Training Examples for 95% Confidence (gamma=0.05): {int(m_required)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
