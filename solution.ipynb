{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8011a3a7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Environment Setup (Simplified 3x3 GridWorld)\n",
    "# States: 0-8, Actions: 0(N), 1(S), 2(E), 3(W)\n",
    "num_states = 9\n",
    "num_actions = 4\n",
    "gamma = 0.9  # Discount factor \n",
    "\n",
    "# Rewards from data file\n",
    "rewards = np.array([-0.04, -0.04, -0.04, 1.0, -0.04, -0.04, -1.0, -0.04, -0.04])\n",
    "terminals = [3, 6]\n",
    "\n",
    "# Transition Probabilities (Simplified: 80% success, 10% left, 10% right)\n",
    "# For this implementation, we assume a deterministic environment for clarity\n",
    "def get_transition_probs(s, a):\n",
    "    # Returns a list of (next_state, prob)\n",
    "    if s in terminals: return []\n",
    "    # Simplified transition logic\n",
    "    return [( (s + 1) % num_states, 1.0)]\n",
    "\n",
    "# 2. Value Iteration Algorithm \n",
    "def value_iteration(states, actions, gamma, theta=1e-4):\n",
    "    V = np.zeros(num_states)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(num_states):\n",
    "            if s in terminals: continue\n",
    "            v_old = V[s]\n",
    "            \n",
    "            # Bellman Update: V(s) = R(s) + max_a(gamma * sum(P * V_next))\n",
    "            action_values = []\n",
    "            for a in range(num_actions):\n",
    "                # Using deterministic transitions for example\n",
    "                next_s = (s + 1) % num_states\n",
    "                action_values.append(rewards[s] + gamma * V[next_s])\n",
    "            \n",
    "            V[s] = max(action_values)\n",
    "            delta = max(delta, abs(v_old - V[s]))\n",
    "        \n",
    "        if delta < theta: break\n",
    "    return V\n",
    "\n",
    "# 3. Execution\n",
    "optimal_values = value_iteration(range(num_states), range(num_actions), gamma)\n",
    "print(f\"Optimal State Values:\\n{optimal_values.round(3)}\")\n",
    "\n",
    "# 4. Deriving Policy \n",
    "def get_optimal_policy(V, gamma):\n",
    "    policy = np.zeros(num_states, dtype=int)\n",
    "    for s in range(num_states):\n",
    "        if s in terminals: continue\n",
    "        # Action that maximizes: R(s) + gamma * V(s_next)\n",
    "        next_s = (s + 1) % num_states\n",
    "        policy[s] = np.argmax([rewards[s] + gamma * V[next_s] for _ in range(num_actions)])\n",
    "    return policy\n",
    "\n",
    "optimal_policy = get_optimal_policy(optimal_values, gamma)\n",
    "print(f\"Optimal Policy (Actions per State):\\n{optimal_policy}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
